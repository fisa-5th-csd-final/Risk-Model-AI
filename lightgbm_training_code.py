# ===============================================================
# ğŸ§  LightGBM ê¸°ë°˜ ê³ ê° ì—°ì²´ ìœ„í—˜ë„ ì˜ˆì¸¡ AI â€” ìµœì í™” ë²„ì „
# ===============================================================

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, roc_curve
import matplotlib.pyplot as plt
import json

# ---------------------------------------------------------------
# 1) ë°ì´í„° ë¡œë“œ
# ---------------------------------------------------------------
df = pd.read_csv('train_dataset.csv')
print(f"âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ: {df.shape[0]:,} rows, {df.shape[1]} cols")
print("ğŸ“Š ì»¬ëŸ¼ ì˜ˆì‹œ:", list(df.columns)[:10])

# ---------------------------------------------------------------
# 2) íƒ€ê¹ƒ ìƒì„± (ì—°ì²´ ì—¬ë¶€: 1íšŒ ì´ìƒì´ë©´ 1)
# ---------------------------------------------------------------
if 'delinquency_count' not in df.columns:
    raise ValueError("âŒ 'delinquency_count' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

df['y'] = (df['delinquency_count'] > 0).astype(int)

# ---------------------------------------------------------------
# 3) X/y ë¶„ë¦¬ ë° ë²”ì£¼í˜• ì²˜ë¦¬
# ---------------------------------------------------------------
drop_cols = ['customer_id', 'delinquency_count', 'y']
X = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')
y = df['y']

# object â†’ category ë¡œ ìºìŠ¤íŒ… (LightGBMì´ ì¹´í…Œê³ ë¦¬ë¡œ ì§ì ‘ ì²˜ë¦¬)
cat_cols = X.select_dtypes(include=['object']).columns.tolist()
for c in cat_cols:
    X[c] = X[c].astype('category')

print(f"âœ… Feature ê°œìˆ˜: {X.shape[1]}ê°œ (ë²”ì£¼í˜• {len(cat_cols)}ê°œ)")

# ---------------------------------------------------------------
# 4) í•™ìŠµ/ê²€ì¦ ë¶„í• 
# ---------------------------------------------------------------
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f"âœ… Train: {X_train.shape}, Valid: {X_val.shape}")

# ---------------------------------------------------------------
# 5) LightGBM Dataset ìƒì„±
# ---------------------------------------------------------------
train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_cols)
val_data   = lgb.Dataset(X_val,   label=y_val,   categorical_feature=cat_cols, reference=train_data)

# í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì • (neg/pos)
pos = int(y_train.sum())
neg = int(len(y_train) - pos)
scale_pos_weight = float(neg) / max(1.0, float(pos))
print(f"ğŸ“ scale_pos_weight = {scale_pos_weight:.2f} (neg={neg}, pos={pos})")

# ---------------------------------------------------------------
# 6) í•˜ì´í¼íŒŒë¼ë¯¸í„° (ê¶Œì¥ ë² ì´ìŠ¤ë¼ì¸)
# ---------------------------------------------------------------
params = {
    'objective': 'binary',
    'metric': ['auc', 'binary_logloss'],
    'boosting_type': 'gbdt',
    'learning_rate': 0.03,
    'num_leaves': 31,
    'max_depth': -1,
    'feature_fraction': 0.85,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'min_data_in_leaf': 50,
    'lambda_l2': 5.0,
    'scale_pos_weight': scale_pos_weight,
    'seed': 42,
    'verbose': -1,
    'num_threads': -1
}

# ---------------------------------------------------------------
# 7) ëª¨ë¸ í•™ìŠµ (EARLY STOPPING)
# ---------------------------------------------------------------
print("\nğŸš€ ëª¨ë¸ í•™ìŠµ ì¤‘...")
model = lgb.train(
    params,
    train_data,
    valid_sets=[train_data, val_data],
    valid_names=['train', 'valid'],
    num_boost_round=5000,
    early_stopping_rounds=200,
    verbose_eval=100
)

# ---------------------------------------------------------------
# 8) ì„±ëŠ¥: AUC + ìµœì  ì„ê³„ê°’ì—ì„œ ë¦¬í¬íŠ¸
# ---------------------------------------------------------------
preds = model.predict(X_val, num_iteration=model.best_iteration)
auc = roc_auc_score(y_val, preds)
print(f"\nğŸ¯ Validation AUC: {auc:.4f}")

# Youdenâ€™s Jë¥¼ ìµœëŒ€í™”í•˜ëŠ” ì„ê³„ê°’
fpr, tpr, thr = roc_curve(y_val, preds)
best_ix = np.argmax(tpr - fpr)
best_thr = float(thr[best_ix])
print(f"ğŸ” Best threshold (Youden's J): {best_thr:.4f}")

y_hat = (preds >= best_thr).astype(int)
print("\nğŸ“Š Classification Report @best_thr\n", classification_report(y_val, y_hat))
print("\nğŸ“‰ Confusion Matrix @best_thr\n", confusion_matrix(y_val, y_hat))

# ---------------------------------------------------------------
# 9) ìœ„í—˜ë„(%) + 5ë‹¨ê³„ ë²„í‚·(ë¶„ìœ„ìˆ˜ ê¸°ë°˜: ê° ë ˆë²¨ ì¸ì›ìˆ˜ ê· í˜•)
# ---------------------------------------------------------------
df_val = pd.DataFrame(index=X_val.index)
df_val['pred_prob']    = preds
df_val['risk_percent'] = (preds * 100).round(1)

# ë¶„ìœ„ìˆ˜ ê²½ê³„ ê³„ì‚° (ì¤‘ë³µ ê²½ê³„ ë³´ì •)
edges = np.quantile(preds, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])
edges = np.unique(np.round(edges, 6))
if len(edges) < 6:  # ì˜ˆì¸¡ì´ ì¹˜ìš°ì¹œ ê²½ìš° ê· ë“± ë¶„í• ë¡œ ëŒ€ì²´
    edges = np.linspace(0, 1, 6)

labels = ['ë§¤ìš° ë‚®ìŒ', 'ë‚®ìŒ', 'ë³´í†µ', 'ë†’ìŒ', 'ë§¤ìš° ë†’ìŒ']
df_val['risk_level'] = pd.cut(preds, bins=edges, labels=labels, include_lowest=True)

# (ì„ íƒ) ê³ ê° ID ë¶™ì´ê¸°
if 'customer_id' in df.columns:
    df_val['customer_id'] = df.loc[df_val.index, 'customer_id']

print("\nğŸ¯ ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ")
print(df_val[['customer_id']].join(df_val[['risk_percent','risk_level']], how='left').head()
      if 'customer_id' in df_val.columns else df_val[['risk_percent','risk_level']].head())

# ---------------------------------------------------------------
# 10) ì¤‘ìš” í”¼ì²˜ ì‹œê°í™”
# ---------------------------------------------------------------
plt.figure(figsize=(10,6))
lgb.plot_importance(model, max_num_features=20, importance_type='gain')
plt.title('Top 20 Feature Importances')
plt.tight_layout()
plt.show()

# ---------------------------------------------------------------
# 11) ì €ì¥: ëª¨ë¸ + ë©”íƒ€(ì„ê³„ê°’/ë²„í‚·/ë²”ì£¼í˜•)
# ---------------------------------------------------------------
model.save_model('lgbm_delinquency_model.txt')
meta = {
    'best_iteration': int(model.best_iteration),
    'best_threshold': best_thr,
    'quantile_edges': [float(e) for e in edges],
    'categorical_features': list(map(str, cat_cols))
}
with open('inference_meta.json', 'w', encoding='utf-8') as f:
    json.dump(meta, f, ensure_ascii=False, indent=2)

print("\nğŸ’¾ ì €ì¥ ì™„ë£Œ â†’ lgbm_delinquency_model.txt, inference_meta.json")
